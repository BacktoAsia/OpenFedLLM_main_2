fed_alg: "local_fedavg_0"
num_clients: 1
sample_clients: 1

model_name_or_path: /disk1/Data/Medical-Ours/Brain_Tumor_data/ckpt/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/392a143b624368100f77a3eafaa4a2468ba50a72
# "/disk1/Data/Medical-Ours/Brain_Tumor_data/ckpt/llama-3-8B/Meta-Llama-3___1-8B"
llm_type: "meta-llama"
vision_tower_type: clip
vision_tower: "/home/weiying/Documents/models--openai--clip-vit-base-patch32/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
# mlp6x_gelu
mm_projector_type: linear
mm_vision_select_layer: -2
mm_use_im_start_end: False
mm_use_im_patch_token: False

dataset_name: "scienceQA"
data_path: /home/weiying/data/SciQA/data/central_training/
image_folder: /home/weiying/data/ScienceQA/image/
pad_token_version: "v1"
image_aspect_ratio: pad
group_by_modality_length: True

use_peft: True
lora_enable: True
peft_lora_r: 8
peft_lora_alpha: 32

lr_scheduler_type: "cosine"
learning_rate: 5e-5
num_train_epochs: 10
# max_steps: 1
num_rounds: 30
batch_size: 4
model_max_length: 1024
gradient_accumulation_steps: 1
weight_decay: 0.0


load_in_4bit: False
load_in_8bit: False
output_dir: "./output_local_mlp"
save_steps: 200
save_total_limit: 4
logging_steps: 5
gradient_checkpointing: True

report_to: tensorboard
evaluation_strategy: epoch

dataset_sample: 20000
template: "alpaca"
bf16: True
tf32: True