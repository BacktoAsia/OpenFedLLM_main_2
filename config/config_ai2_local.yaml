fed_alg: "local_fedavg_0"
num_clients: 1
sample_clients: 1

model_name_or_path: "/home/u3011649/Documents/llama3.2/392a143b624368100f77a3eafaa4a2468ba50a72/"
# "/disk1/Data/Medical-Ours/Brain_Tumor_data/ckpt/llama-3-8B/Meta-Llama-3___1-8B"
llm_type: "meta-llama"
vision_tower_type: clip
vision_tower: "/home/u3011649/Documents/clip/"
# mlp6x_gelu
mm_projector_type: attention
mm_vision_select_layer: -2
mm_use_im_start_end: False
mm_use_im_patch_token: False

dataset_name: "scienceQA"
data_path: /home/u3011649/Documents/data/SciQA/data/central_training/
split_strategy: "central"

image_folder: /home/u3011649/Documents/data/SciQA/data/image/
pad_token_version: "v1"
image_aspect_ratio: pad
group_by_modality_length: True

use_peft: True
lora_enable: True
peft_lora_r: 8
peft_lora_alpha: 32

lr_scheduler_type: "cosine"
learning_rate: 1e-4
num_train_epochs: 10
# max_steps: 1
num_rounds: 30
batch_size: 4
model_max_length: 1024
gradient_accumulation_steps: 1
weight_decay: 0.0


load_in_4bit: False
load_in_8bit: False
output_dir: "./output_local_mlp"
save_steps: 200
save_total_limit: 4
logging_steps: 5
gradient_checkpointing: True

report_to: tensorboard
evaluation_strategy: epoch

dataset_sample: 20000
template: "alpaca"
bf16: True
tf32: True