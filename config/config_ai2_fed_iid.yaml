fed_alg: "fedavg"
num_clients: 10
sample_clients: 5

model_name_or_path: "/home/u3011649/Documents/llama3.2/392a143b624368100f77a3eafaa4a2468ba50a72/"
llm_type: "meta-llama"
vision_tower_type: clip
vision_tower: "/home/u3011649/Documents/clip/"
# mlp2x_gelu
mm_projector_type: linear
mm_vision_select_layer: -2
mm_use_im_start_end: False
mm_use_im_patch_token: False


dataset_name: "scienceQA"
# data_path: /home/u3011649/Documents/data/SciQA/data/clients/non_iid_clients/QCM-A/
split_strategy: "iid"
data_path: /home/u3011649/Documents/data/SciQA/data/clients/iid_clients/QCM-A/

image_folder: /home/u3011649/Documents/data/SciQA/data/image/
pad_token_version: "v1"
image_aspect_ratio: pad
group_by_modality_length: True

output_dir: "./output_fed_iid_linear"

lr_scheduler_type: "cosine"
learning_rate: 1e-4
num_train_epochs: 10
# max_steps: None
num_rounds: 30
batch_size: 4
model_max_length: 1024
gradient_accumulation_steps: 1
weight_decay: 0.0

use_peft: True
lora_enable: True
peft_lora_r: 8
peft_lora_alpha: 32


load_in_4bit: False
load_in_8bit: False
save_steps: 100
save_total_limit: 4
logging_steps: 10
gradient_checkpointing: True

report_to: tensorboard
evaluation_strategy: epoch

dataset_sample: 20000
template: "alpaca"
bf16: True
tf32: True